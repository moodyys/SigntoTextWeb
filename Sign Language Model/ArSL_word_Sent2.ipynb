{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1749614729.351744 41475700 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M4 Pro\n"
     ]
    }
   ],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.7, max_num_hands = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mediapipe_detection' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 62\u001b[39m\n",
      "\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Perform Hand Detection\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m image, results = \u001b[43mmediapipe_detection\u001b[49m(frame, hands)\n",
      "\u001b[32m     64\u001b[39m predicted_character = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Default empty prediction\u001b[39;00m\n",
      "\u001b[32m     65\u001b[39m backspace_detected = \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# Default backspace flag\u001b[39;00m\n",
      "\n",
      "\u001b[31mNameError\u001b[39m: name 'mediapipe_detection' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from keras.models import load_model\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# Load Model\n",
    "modelLSTM = load_model('/Users/ahmedyouness/Sign Language Model/AsL_detection.h5')\n",
    "\n",
    "# Initialize Webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Define Label Mapping (Arabic Letters)\n",
    "label_map = {\n",
    "    0: 'ع', 1: 'ا', 2: 'ب', 3: 'ض', 4: 'د', 5: 'ف',\n",
    "    6: 'غ', 7: 'ح', 8: 'ه', 9: 'ج', 10: 'ك', 11: 'خ',\n",
    "    12: 'ل', 13: 'م', 14: 'ن', 15: 'ق', 16: 'ر', 17: 'ص',\n",
    "    18: 'س', 19: 'ش', 20: 'ط', 21: 'ت', 22: 'ذ', 23: 'ث',\n",
    "    24: 'و', 25: 'ي', 26: 'ظ', 27: 'ز'\n",
    "}\n",
    "\n",
    "# Buffers and Variables\n",
    "letter_buffer = []  # Stores detected letters\n",
    "word_buffer = []  # Stores words for sentence\n",
    "last_letter = None  # Last detected letter (for debounce)\n",
    "current_word = \"\"  # Word being formed\n",
    "current_sentence = \"\"  # Sentence being formed\n",
    "last_detected_time = time.time()\n",
    "detection_count = {}  # Track repeated letter detections\n",
    "\n",
    "# Accuracy & Delay Settings\n",
    "CONFIDENCE_THRESHOLD = 0.85  # Higher threshold to prevent mistakes\n",
    "DETECTION_REPEAT = 3  # Letter must be detected this many times before adding\n",
    "LETTER_DELAY = 0.5  # Seconds before allowing a new letter\n",
    "\n",
    "# Load Arabic Font (Make sure you have an Arabic font file)\n",
    "ARABIC_FONT_PATH = \"/System/Library/Fonts/Supplemental/Arial Unicode.ttf\"  # Change this to a valid Arabic font file path\n",
    "\n",
    "def render_arabic_text(text):\n",
    "    \"\"\"Fixes Arabic text rendering for OpenCV using Pillow.\"\"\"\n",
    "    reshaped_text = arabic_reshaper.reshape(text)  # Fix Arabic letter order\n",
    "    bidi_text = get_display(reshaped_text)  # Fix right-to-left direction\n",
    "    return bidi_text\n",
    "\n",
    "def draw_text_with_pil(image, text, position, font_path, font_size=32, color=(255, 255, 255)):\n",
    "    \"\"\"Draw Arabic text on an OpenCV image using PIL.\"\"\"\n",
    "    pil_image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    draw = ImageDraw.Draw(pil_image)\n",
    "    font = ImageFont.truetype(font_path, font_size)\n",
    "    draw.text(position, text, font=font, fill=color)\n",
    "    return cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Could not read frame\")\n",
    "        break\n",
    "\n",
    "    # Perform Hand Detection\n",
    "    image, results = mediapipe_detection(frame, hands)\n",
    "\n",
    "    predicted_character = \"\"  # Default empty prediction\n",
    "    backspace_detected = False  # Default backspace flag\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        draw_styled_landmarks(frame, results)  # Draw hand landmarks\n",
    "\n",
    "        keypoints = extract_keypoints(results)\n",
    "        keypoints = np.array(keypoints[:42]).reshape(1, -1)  # Use only first hand’s keypoints\n",
    "\n",
    "        # Check for backspace (second hand detected)\n",
    "        if len(results.multi_hand_landmarks) > 1:\n",
    "            backspace_detected = True\n",
    "\n",
    "        # Make prediction\n",
    "        prediction = modelLSTM.predict(keypoints)\n",
    "        predicted_class = np.argmax(prediction)\n",
    "        confidence = prediction[0, predicted_class]  # Extract confidence\n",
    "\n",
    "        # If confidence is high, process the character\n",
    "        if confidence > CONFIDENCE_THRESHOLD:\n",
    "            predicted_character = label_map[int(predicted_class)]\n",
    "\n",
    "            # Count repeated detections of the same letter\n",
    "            if predicted_character in detection_count:\n",
    "                detection_count[predicted_character] += 1\n",
    "            else:\n",
    "                detection_count[predicted_character] = 1\n",
    "\n",
    "            # If the letter is detected consistently, add it to the buffer\n",
    "            if detection_count[predicted_character] >= DETECTION_REPEAT:\n",
    "                if predicted_character != last_letter and (time.time() - last_detected_time) > LETTER_DELAY:\n",
    "                    letter_buffer.append(predicted_character)\n",
    "                    last_detected_time = time.time()  # Reset timer\n",
    "                    last_letter = predicted_character  # Update last letter\n",
    "                    detection_count.clear()  # Reset count to avoid duplicates\n",
    "\n",
    "    # Handle Backspace Gesture (Raise Second Hand)\n",
    "    if backspace_detected:\n",
    "        if letter_buffer:\n",
    "            print(\"Backspace Detected! Removing last letter.\")\n",
    "            letter_buffer.pop()  # Remove last letter\n",
    "        elif word_buffer:\n",
    "            print(\"Backspace Detected! Removing last word.\")\n",
    "            word_buffer.pop()  # Remove last word\n",
    "        last_detected_time = time.time()  # Reset timing\n",
    "\n",
    "    # Check for a pause (indicating end of word)\n",
    "    current_time = time.time()\n",
    "    if current_time - last_detected_time > 3:  # 1.5 seconds of no new letter\n",
    "        if letter_buffer:\n",
    "            current_word = \"\".join(letter_buffer)  # Form word\n",
    "            word_buffer.append(current_word)  # Add word to sentence\n",
    "            letter_buffer.clear()  # Clear letter buffer\n",
    "            print(f\"Word Detected: {current_word}\")\n",
    "\n",
    "    # Form full sentence\n",
    "    current_sentence = \" \".join(word_buffer)\n",
    "\n",
    "    # Fix Arabic text rendering\n",
    "    display_word = render_arabic_text(current_word)\n",
    "    display_sentence = render_arabic_text(current_sentence)\n",
    "    display_letter = render_arabic_text(predicted_character)\n",
    "\n",
    "    # Draw a black rectangle as background for text\n",
    "    cv2.rectangle(frame, (0, 0), (640, 120), (0, 0, 0), -1)  # Black background\n",
    "\n",
    "    # Use PIL to draw Arabic text correctly\n",
    "    frame = draw_text_with_pil(frame, f\"Sentence: {display_sentence}\", (50, 10), ARABIC_FONT_PATH, 32, (255, 255, 255))\n",
    "    frame = draw_text_with_pil(frame, f\"Word: {display_word}\", (50, 50), ARABIC_FONT_PATH, 32, (255, 255, 255))\n",
    "    frame = draw_text_with_pil(frame, f\"Letter: {display_letter}\", (50, 90), ARABIC_FONT_PATH, 32, (0, 255, 0))\n",
    "\n",
    "    # Show video output\n",
    "    cv2.imshow('Real-Time Sign Language Recognition', frame)\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break  # Press 'q' to exit\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1749614729.357886 41479212 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw pose connections\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image,  # image to draw\n",
    "                hand_landmarks,  # model output\n",
    "                mp_hands.HAND_CONNECTIONS,  # hand connections\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Reh',\n",
       " 'Qaf',\n",
       " 'Jeem',\n",
       " 'Theh',\n",
       " 'Waw',\n",
       " 'Sad',\n",
       " 'Lam',\n",
       " 'Ghain',\n",
       " 'Ain',\n",
       " 'Kaf',\n",
       " 'Alef',\n",
       " 'Teh',\n",
       " 'Seen',\n",
       " 'Feh',\n",
       " 'Khah',\n",
       " 'Zain',\n",
       " 'Noon',\n",
       " 'Beh',\n",
       " 'Heh',\n",
       " 'Dad',\n",
       " 'Sheen',\n",
       " 'Hah',\n",
       " 'Dal',\n",
       " 'Meem',\n",
       " 'Thal',\n",
       " 'Yeh',\n",
       " 'Zah',\n",
       " 'Tah']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = r\"/Users/ahmedyouness/Sign Language Model/char_data\"\n",
    "actions = []\n",
    "for dir_ in os.listdir(DATA_DIR):\n",
    "    actions.append(dir_)\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.0\n",
      "2.19.0\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "print(keras.__version__)\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "modelLSTM = load_model(\"/Users/ahmedyouness/AndroidStudioProjects/Signtotext/Sign Language Model/AsL_detection.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mediapipe_detection' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 62\u001b[39m\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Perform Hand Detection\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m image, results = \u001b[43mmediapipe_detection\u001b[49m(frame, hands)\n\u001b[32m     64\u001b[39m predicted_character = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Default empty prediction\u001b[39;00m\n\u001b[32m     65\u001b[39m backspace_detected = \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# Default backspace flag\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'mediapipe_detection' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from keras.models import load_model\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# Load Model\n",
    "modelLSTM = load_model('/Users/ahmedyouness/Sign Language Model/AsL_detection.h5')\n",
    "\n",
    "# Initialize Webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Define Label Mapping (Arabic Letters)\n",
    "label_map = {\n",
    "    0: 'ع', 1: 'ا', 2: 'ب', 3: 'ض', 4: 'د', 5: 'ف',\n",
    "    6: 'غ', 7: 'ح', 8: 'ه', 9: 'ج', 10: 'ك', 11: 'خ',\n",
    "    12: 'ل', 13: 'م', 14: 'ن', 15: 'ق', 16: 'ر', 17: 'ص',\n",
    "    18: 'س', 19: 'ش', 20: 'ط', 21: 'ت', 22: 'ذ', 23: 'ث',\n",
    "    24: 'و', 25: 'ي', 26: 'ظ', 27: 'ز'\n",
    "}\n",
    "\n",
    "# Buffers and Variables\n",
    "letter_buffer = []  # Stores detected letters\n",
    "word_buffer = []  # Stores words for sentence\n",
    "last_letter = None  # Last detected letter (for debounce)\n",
    "current_word = \"\"  # Word being formed\n",
    "current_sentence = \"\"  # Sentence being formed\n",
    "last_detected_time = time.time()\n",
    "detection_count = {}  # Track repeated letter detections\n",
    "\n",
    "# Accuracy & Delay Settings\n",
    "CONFIDENCE_THRESHOLD = 0.85  # Higher threshold to prevent mistakes\n",
    "DETECTION_REPEAT = 3  # Letter must be detected this many times before adding\n",
    "LETTER_DELAY = 0.5  # Seconds before allowing a new letter\n",
    "\n",
    "# Load Arabic Font (Make sure you have an Arabic font file)\n",
    "ARABIC_FONT_PATH = \"/System/Library/Fonts/Supplemental/Arial Unicode.ttf\"  # Change this to a valid Arabic font file path\n",
    "\n",
    "def render_arabic_text(text):\n",
    "    \"\"\"Fixes Arabic text rendering for OpenCV using Pillow.\"\"\"\n",
    "    reshaped_text = arabic_reshaper.reshape(text)  # Fix Arabic letter order\n",
    "    bidi_text = get_display(reshaped_text)  # Fix right-to-left direction\n",
    "    return bidi_text\n",
    "\n",
    "def draw_text_with_pil(image, text, position, font_path, font_size=32, color=(255, 255, 255)):\n",
    "    \"\"\"Draw Arabic text on an OpenCV image using PIL.\"\"\"\n",
    "    pil_image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    draw = ImageDraw.Draw(pil_image)\n",
    "    font = ImageFont.truetype(font_path, font_size)\n",
    "    draw.text(position, text, font=font, fill=color)\n",
    "    return cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Could not read frame\")\n",
    "        break\n",
    "\n",
    "    # Perform Hand Detection\n",
    "    image, results = mediapipe_detection(frame, hands)\n",
    "\n",
    "    predicted_character = \"\"  # Default empty prediction\n",
    "    backspace_detected = False  # Default backspace flag\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        draw_styled_landmarks(frame, results)  # Draw hand landmarks\n",
    "\n",
    "        keypoints = extract_keypoints(results)\n",
    "        keypoints = np.array(keypoints[:42]).reshape(1, -1)  # Use only first hand’s keypoints\n",
    "\n",
    "        # Check for backspace (second hand detected)\n",
    "        if len(results.multi_hand_landmarks) > 1:\n",
    "            backspace_detected = True\n",
    "\n",
    "        # Make prediction\n",
    "        prediction = modelLSTM.predict(keypoints)\n",
    "        predicted_class = np.argmax(prediction)\n",
    "        confidence = prediction[0, predicted_class]  # Extract confidence\n",
    "\n",
    "        # If confidence is high, process the character\n",
    "        if confidence > CONFIDENCE_THRESHOLD:\n",
    "            predicted_character = label_map[int(predicted_class)]\n",
    "\n",
    "            # Count repeated detections of the same letter\n",
    "            if predicted_character in detection_count:\n",
    "                detection_count[predicted_character] += 1\n",
    "            else:\n",
    "                detection_count[predicted_character] = 1\n",
    "\n",
    "            # If the letter is detected consistently, add it to the buffer\n",
    "            if detection_count[predicted_character] >= DETECTION_REPEAT:\n",
    "                if predicted_character != last_letter and (time.time() - last_detected_time) > LETTER_DELAY:\n",
    "                    letter_buffer.append(predicted_character)\n",
    "                    last_detected_time = time.time()  # Reset timer\n",
    "                    last_letter = predicted_character  # Update last letter\n",
    "                    detection_count.clear()  # Reset count to avoid duplicates\n",
    "\n",
    "    # Handle Backspace Gesture (Raise Second Hand)\n",
    "    if backspace_detected:\n",
    "        if letter_buffer:\n",
    "            print(\"Backspace Detected! Removing last letter.\")\n",
    "            letter_buffer.pop()  # Remove last letter\n",
    "        elif word_buffer:\n",
    "            print(\"Backspace Detected! Removing last word.\")\n",
    "            word_buffer.pop()  # Remove last word\n",
    "        last_detected_time = time.time()  # Reset timing\n",
    "\n",
    "    # Check for a pause (indicating end of word)\n",
    "    current_time = time.time()\n",
    "    if current_time - last_detected_time > 3:  # 1.5 seconds of no new letter\n",
    "        if letter_buffer:\n",
    "            current_word = \"\".join(letter_buffer)  # Form word\n",
    "            word_buffer.append(current_word)  # Add word to sentence\n",
    "            letter_buffer.clear()  # Clear letter buffer\n",
    "            print(f\"Word Detected: {current_word}\")\n",
    "\n",
    "    # Form full sentence\n",
    "    current_sentence = \" \".join(word_buffer)\n",
    "\n",
    "    # Fix Arabic text rendering\n",
    "    display_word = render_arabic_text(current_word)\n",
    "    display_sentence = render_arabic_text(current_sentence)\n",
    "    display_letter = render_arabic_text(predicted_character)\n",
    "\n",
    "    # Draw a black rectangle as background for text\n",
    "    cv2.rectangle(frame, (0, 0), (640, 120), (0, 0, 0), -1)  # Black background\n",
    "\n",
    "    # Use PIL to draw Arabic text correctly\n",
    "    frame = draw_text_with_pil(frame, f\"Sentence: {display_sentence}\", (50, 10), ARABIC_FONT_PATH, 32, (255, 255, 255))\n",
    "    frame = draw_text_with_pil(frame, f\"Word: {display_word}\", (50, 50), ARABIC_FONT_PATH, 32, (255, 255, 255))\n",
    "    frame = draw_text_with_pil(frame, f\"Letter: {display_letter}\", (50, 90), ARABIC_FONT_PATH, 32, (0, 255, 0))\n",
    "\n",
    "    # Show video output\n",
    "    cv2.imshow('Real-Time Sign Language Recognition', frame)\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break  # Press 'q' to exit\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/cj/d5mr3czd1zj6_718j3t120gw0000gn/T/tmpg7i9h8i2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/cj/d5mr3czd1zj6_718j3t120gw0000gn/T/tmpg7i9h8i2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/var/folders/cj/d5mr3czd1zj6_718j3t120gw0000gn/T/tmpg7i9h8i2'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 42), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 28), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  14233429264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14233432144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14233429072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14233433680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14233435984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14233435216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14233435024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14233430608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14233430800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14233434064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14233424080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  14233431760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1749591867.803865 40880787 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1749591867.804020 40880787 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion successful!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1749591867.807789 40880787 mlir_graph_optimization_pass.cc:425] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the Keras .h5 model\n",
    "model = tf.keras.models.load_model(\"/Users/ahmedyouness/Sign Language Model/AsL_detection.h5\")\n",
    "\n",
    "# Convert to TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TFLite model\n",
    "with open(\"/Users/ahmedyouness/Sign Language Model/AsL_detection.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"Conversion successful!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
